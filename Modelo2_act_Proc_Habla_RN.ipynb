{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romanzelararg/Tecnicas-de-Procesamiento-del-Habla-Grupo11/blob/main/Modelo2_act_Proc_Habla_RN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELO DE RED NEURONAL CON CAPAS DENSAS.\n",
        "\n",
        "#**Procesamiento de Texto**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zhaTYEQyIKxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparación del área de trabajo"
      ],
      "metadata": {
        "id": "EI-pH6ftIbVV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78QGuAA9MrW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar Librerias"
      ],
      "metadata": {
        "id": "Iu7vfoYXxm2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install unidecode\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WtLPhNGfABbI",
        "outputId": "9e77b4d6-85aa-4d10-896e-6bc42d744c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # Se agrega esta importación para manejar archivos\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud  # Se agrega esta importación para la nube de palabras\n"
      ],
      "metadata": {
        "id": "NxFO-fxCxsQX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar recursos de NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fupPQWTqS5WP",
        "outputId": "863d3636-b887-4c06-e398-eda350db7279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar Documento"
      ],
      "metadata": {
        "id": "z74T6Xrox3Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar si la ruta existe\n",
        "if os.path.exists(libro):\n",
        "    print(\"El archivo existe y la ruta es correcta.\")\n",
        "else:\n",
        "    print(\"El archivo no existe o la ruta es incorrecta.\")\n",
        "\n",
        "# Leer y limpiar el texto\n",
        "with open(libro, 'r') as f:\n",
        "    texto = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcNqM52x57r",
        "outputId": "24613d39-7fa4-4327-f1a4-6786beabb158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo existe y la ruta es correcta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesamiento y Análisis de Sentimiento"
      ],
      "metadata": {
        "id": "AcGEgNRXjO6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza y Tokenización de texto"
      ],
      "metadata": {
        "id": "PzVfYldlArrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de limpieza de texto\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'\\W', ' ', texto)\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
        "    texto = re.sub(r'\\@\\w+|\\#', '', texto)\n",
        "    return texto"
      ],
      "metadata": {
        "id": "u5MLemWkAv5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para lematizar texto\n",
        "def lematizar_texto(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens_lemmatizados = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens_lemmatizados"
      ],
      "metadata": {
        "id": "ZlzCkCVcjy7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_limpio = limpiar_texto(texto)"
      ],
      "metadata": {
        "id": "vR4A-VFArm5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización\n",
        "tokens = nltk.word_tokenize(texto_limpio)\n",
        "\n",
        "print('Cantidad de tokens unicos',len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541Ga8hNrpXt",
        "outputId": "f46480ba-16b5-426d-9146-b6cacc2fba6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tokens unicos 22637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fraccion de Stopwords en un corpus\n",
        "\n",
        "def stopwords_percentage(text):\n",
        "  '''\n",
        "  aqui usamos un recurso léxico (stopwords) para filtrar un corpus\n",
        "  '''\n",
        "  stopwd = stopwords.words('spanish')\n",
        "  content = [w  for w in text if w.lower() not in stopwd]\n",
        "  return len(content)/len(text)\n",
        "\n",
        "stopwords_percentage(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVyK8hA-rsEv",
        "outputId": "53bd9a93-be8c-4b78-cf0c-fdc24e6dfddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5240535406635155"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Limpieza de texto\n",
        "\n",
        "# Eliminar stopwords\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "tokens_filtrados = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "print('Cantidad de tokens sin stopwords',len(tokens_filtrados))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds7cyQHQrx0q",
        "outputId": "f0df1f63-911b-462a-ed2d-b48b2b5797d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tokens sin stopwords 11863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematización\n",
        "tokens_lemmatizados = lematizar_texto(tokens_filtrados)"
      ],
      "metadata": {
        "id": "4eEQEsALr6iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar el analizador de sentimiento VADER\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "kjg3ugDKr8_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analizar sentimiento de los tokens\n",
        "def analizar_sentimiento(tokens):\n",
        "    etiquetas_sentimiento = []\n",
        "    for token in tokens:\n",
        "        score = analyzer.polarity_scores(token)\n",
        "        if score['compound'] >= 0.05:\n",
        "            etiquetas_sentimiento.append(1)  # Positivo\n",
        "        elif score['compound'] <= -0.05:\n",
        "            etiquetas_sentimiento.append(-1)  # Negativo\n",
        "        else:\n",
        "            etiquetas_sentimiento.append(0)  # Neutral\n",
        "    return etiquetas_sentimiento\n"
      ],
      "metadata": {
        "id": "3_G4-FWkr-6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener etiquetas de sentimiento para los tokens\n",
        "etiquetas_sentimiento = analizar_sentimiento(tokens_lemmatizados)\n",
        "\n",
        "# Convertir a numpy array\n",
        "y = np.array(etiquetas_sentimiento)\n",
        "\n",
        "print('Tamaño de y:', y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTmr71irsA1Y",
        "outputId": "66a6cc64-ae4a-4db2-ad2b-b858e3a9a7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de y: (11863,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construcción del Modelo de Red Neuronal"
      ],
      "metadata": {
        "id": "76PnH2NWle5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(tokens_lemmatizados)\n",
        "\n",
        "print('Tamaño de la matriz TF-IDF:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14A6qvsTR9hd",
        "outputId": "53c763e8-6cd2-444e-d7f0-2fa54d0a2277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de la matriz TF-IDF: (11863, 4920)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asegurar que las longitudes de X e y son consistentes\n",
        "\n"
      ],
      "metadata": {
        "id": "S2bOrBDYTht4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if X.shape[0] != len(y):\n",
        "    raise ValueError(f\"Dimensiones inconsistentes: X tiene {X.shape[0]} muestras, pero y tiene {len(y)} muestras.\")"
      ],
      "metadata": {
        "id": "V6TNnS_0dIvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Construcción del Modelo de Red Neuronal"
      ],
      "metadata": {
        "id": "pvqgKhTGDbnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# División en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "FFUviHeOSBae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Balancear clases usando SMOTE\n",
        "smote = SMOTE()\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "OHvcA7-fTrnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividir los datos en conjuntos de entrenamiento y prueba"
      ],
      "metadata": {
        "id": "dhKczYuKg__A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizacion"
      ],
      "metadata": {
        "id": "M7YZ5LP_DmnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización de los datos de entrada\n",
        "scaler = MaxAbsScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train_resampled.toarray())\n",
        "X_test_normalized = scaler.transform(X_test.toarray())"
      ],
      "metadata": {
        "id": "TW0vnWf4DlhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquitectura del modelo"
      ],
      "metadata": {
        "id": "ghOtbDVY1D0i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrf-gXBMxf5-",
        "outputId": "3f466c01-dfe4-48f2-f2d5-cae6d2817e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 128)               629888    \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 638209 (2.43 MB)\n",
            "Trainable params: 638209 (2.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Definir la arquitectura del modelo\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(X_train_normalized.shape[1],), activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Resumen del modelo\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "hxTkZIIr1JKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entranamiento"
      ],
      "metadata": {
        "id": "Hjbqqsj0GA03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "history = model.fit(X_train_normalized, y_train_resampled, epochs=50, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(X_test_normalized, y_test)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mey6TYXO1OHk",
        "outputId": "12fbbbd0-9791-48aa-b1a0-eefbda5adb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "177/177 [==============================] - 6s 30ms/step - loss: -13.6705 - accuracy: 0.4300 - val_loss: 6.9082 - val_accuracy: 0.4227\n",
            "Epoch 2/50\n",
            "177/177 [==============================] - 4s 20ms/step - loss: -301.5914 - accuracy: 0.4919 - val_loss: 44.1799 - val_accuracy: 0.5243\n",
            "Epoch 3/50\n",
            "177/177 [==============================] - 3s 19ms/step - loss: -1467.9038 - accuracy: 0.4999 - val_loss: 138.8980 - val_accuracy: 0.5551\n",
            "Epoch 4/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -4075.1003 - accuracy: 0.5013 - val_loss: 313.4719 - val_accuracy: 0.5699\n",
            "Epoch 5/50\n",
            "177/177 [==============================] - 4s 24ms/step - loss: -8547.4893 - accuracy: 0.5031 - val_loss: 604.5681 - val_accuracy: 0.5683\n",
            "Epoch 6/50\n",
            "177/177 [==============================] - 4s 21ms/step - loss: -15272.6182 - accuracy: 0.5043 - val_loss: 1038.6909 - val_accuracy: 0.5611\n",
            "Epoch 7/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -24546.1895 - accuracy: 0.5026 - val_loss: 1537.1132 - val_accuracy: 0.5703\n",
            "Epoch 8/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -36482.9570 - accuracy: 0.5040 - val_loss: 2232.9124 - val_accuracy: 0.5712\n",
            "Epoch 9/50\n",
            "177/177 [==============================] - 4s 23ms/step - loss: -51922.1094 - accuracy: 0.5025 - val_loss: 3033.9158 - val_accuracy: 0.5756\n",
            "Epoch 10/50\n",
            "177/177 [==============================] - 4s 25ms/step - loss: -70276.2578 - accuracy: 0.5050 - val_loss: 4130.9678 - val_accuracy: 0.5692\n",
            "Epoch 11/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -91885.8828 - accuracy: 0.5038 - val_loss: 5264.1294 - val_accuracy: 0.5736\n",
            "Epoch 12/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -116931.5547 - accuracy: 0.5037 - val_loss: 6687.6406 - val_accuracy: 0.5727\n",
            "Epoch 13/50\n",
            "177/177 [==============================] - 3s 20ms/step - loss: -145937.7500 - accuracy: 0.5055 - val_loss: 8291.0176 - val_accuracy: 0.5706\n",
            "Epoch 14/50\n",
            "177/177 [==============================] - 5s 29ms/step - loss: -179384.0469 - accuracy: 0.5025 - val_loss: 9916.5449 - val_accuracy: 0.5734\n",
            "Epoch 15/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -215396.3281 - accuracy: 0.5044 - val_loss: 11965.9551 - val_accuracy: 0.5708\n",
            "Epoch 16/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -256909.3906 - accuracy: 0.5040 - val_loss: 14148.0176 - val_accuracy: 0.5713\n",
            "Epoch 17/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -301875.8125 - accuracy: 0.5032 - val_loss: 16347.7109 - val_accuracy: 0.5726\n",
            "Epoch 18/50\n",
            "177/177 [==============================] - 5s 26ms/step - loss: -350524.6875 - accuracy: 0.5024 - val_loss: 18796.6270 - val_accuracy: 0.5742\n",
            "Epoch 19/50\n",
            "177/177 [==============================] - 4s 22ms/step - loss: -405411.2500 - accuracy: 0.5050 - val_loss: 22074.6602 - val_accuracy: 0.5687\n",
            "Epoch 20/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -462509.3438 - accuracy: 0.5030 - val_loss: 25067.3926 - val_accuracy: 0.5703\n",
            "Epoch 21/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -524295.4375 - accuracy: 0.5028 - val_loss: 28210.5684 - val_accuracy: 0.5706\n",
            "Epoch 22/50\n",
            "177/177 [==============================] - 4s 22ms/step - loss: -590564.9375 - accuracy: 0.5040 - val_loss: 31706.5488 - val_accuracy: 0.5706\n",
            "Epoch 23/50\n",
            "177/177 [==============================] - 4s 24ms/step - loss: -660423.6250 - accuracy: 0.5045 - val_loss: 35492.2266 - val_accuracy: 0.5697\n",
            "Epoch 24/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -740217.8750 - accuracy: 0.5022 - val_loss: 39286.3594 - val_accuracy: 0.5704\n",
            "Epoch 25/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -817755.8750 - accuracy: 0.5043 - val_loss: 43290.9414 - val_accuracy: 0.5712\n",
            "Epoch 26/50\n",
            "177/177 [==============================] - 4s 20ms/step - loss: -898573.9375 - accuracy: 0.5020 - val_loss: 47218.3711 - val_accuracy: 0.5738\n",
            "Epoch 27/50\n",
            "177/177 [==============================] - 4s 25ms/step - loss: -994329.1875 - accuracy: 0.5046 - val_loss: 52176.5781 - val_accuracy: 0.5720\n",
            "Epoch 28/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -1083115.1250 - accuracy: 0.5035 - val_loss: 57100.1016 - val_accuracy: 0.5726\n",
            "Epoch 29/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -1182164.5000 - accuracy: 0.5031 - val_loss: 61877.7656 - val_accuracy: 0.5743\n",
            "Epoch 30/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -1294498.6250 - accuracy: 0.5029 - val_loss: 67490.7500 - val_accuracy: 0.5736\n",
            "Epoch 31/50\n",
            "177/177 [==============================] - 5s 28ms/step - loss: -1410456.2500 - accuracy: 0.5044 - val_loss: 73906.9062 - val_accuracy: 0.5712\n",
            "Epoch 32/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -1509691.1250 - accuracy: 0.5023 - val_loss: 79059.3594 - val_accuracy: 0.5726\n",
            "Epoch 33/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -1631708.7500 - accuracy: 0.5038 - val_loss: 85325.8906 - val_accuracy: 0.5720\n",
            "Epoch 34/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -1763971.6250 - accuracy: 0.5034 - val_loss: 91859.8125 - val_accuracy: 0.5724\n",
            "Epoch 35/50\n",
            "177/177 [==============================] - 5s 26ms/step - loss: -1887587.2500 - accuracy: 0.5050 - val_loss: 99084.7188 - val_accuracy: 0.5719\n",
            "Epoch 36/50\n",
            "177/177 [==============================] - 3s 19ms/step - loss: -2041940.0000 - accuracy: 0.5055 - val_loss: 107311.7969 - val_accuracy: 0.5692\n",
            "Epoch 37/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -2182242.0000 - accuracy: 0.5025 - val_loss: 114328.3359 - val_accuracy: 0.5710\n",
            "Epoch 38/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -2331167.7500 - accuracy: 0.5042 - val_loss: 122185.3594 - val_accuracy: 0.5699\n",
            "Epoch 39/50\n",
            "177/177 [==============================] - 4s 25ms/step - loss: -2483986.2500 - accuracy: 0.5034 - val_loss: 130026.7656 - val_accuracy: 0.5706\n",
            "Epoch 40/50\n",
            "177/177 [==============================] - 4s 22ms/step - loss: -2644966.5000 - accuracy: 0.5027 - val_loss: 138218.7031 - val_accuracy: 0.5706\n",
            "Epoch 41/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -2806250.0000 - accuracy: 0.5027 - val_loss: 146040.6875 - val_accuracy: 0.5719\n",
            "Epoch 42/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -2979323.5000 - accuracy: 0.5044 - val_loss: 155159.0469 - val_accuracy: 0.5713\n",
            "Epoch 43/50\n",
            "177/177 [==============================] - 4s 21ms/step - loss: -3170326.5000 - accuracy: 0.5031 - val_loss: 164617.0000 - val_accuracy: 0.5715\n",
            "Epoch 44/50\n",
            "177/177 [==============================] - 5s 26ms/step - loss: -3337161.0000 - accuracy: 0.5060 - val_loss: 175254.8438 - val_accuracy: 0.5704\n",
            "Epoch 45/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -3527377.2500 - accuracy: 0.5035 - val_loss: 184716.9531 - val_accuracy: 0.5708\n",
            "Epoch 46/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -3709723.5000 - accuracy: 0.5039 - val_loss: 194622.8594 - val_accuracy: 0.5710\n",
            "Epoch 47/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -3934692.0000 - accuracy: 0.5024 - val_loss: 204346.0312 - val_accuracy: 0.5720\n",
            "Epoch 48/50\n",
            "177/177 [==============================] - 5s 27ms/step - loss: -4151059.2500 - accuracy: 0.5038 - val_loss: 215418.2500 - val_accuracy: 0.5715\n",
            "Epoch 49/50\n",
            "177/177 [==============================] - 3s 18ms/step - loss: -4357858.5000 - accuracy: 0.5027 - val_loss: 226148.8438 - val_accuracy: 0.5719\n",
            "Epoch 50/50\n",
            "177/177 [==============================] - 3s 17ms/step - loss: -4568674.5000 - accuracy: 0.5044 - val_loss: 237907.3125 - val_accuracy: 0.5715\n",
            "75/75 [==============================] - 0s 3ms/step - loss: -30621.9668 - accuracy: 0.9928\n",
            "Accuracy: 0.992836058139801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matiz de Confusion"
      ],
      "metadata": {
        "id": "PaGntrQa7sZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar predicciones\n",
        "y_pred = model.predict(X_test_normalized)\n",
        "\n",
        "# Convertir las predicciones a etiquetas binarias\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Mostrar reporte de clasificación\n",
        "print(\"Reporte de Clasificación:\")\n",
        "print(classification_report(y_test, y_pred_binary, target_names = ['Negativo', 'Positivo', 'Neutral']))\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "print(\"Matriz de Confusión:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ovpoxRmsxqX",
        "outputId": "e4eb006c-f123-493a-c56d-f9f50867f28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 0s 3ms/step\n",
            "Reporte de Clasificación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negativo       0.00      0.00      0.00         9\n",
            "    Positivo       0.99      1.00      1.00      2351\n",
            "     Neutral       1.00      0.38      0.56        13\n",
            "\n",
            "    accuracy                           0.99      2373\n",
            "   macro avg       0.66      0.46      0.52      2373\n",
            "weighted avg       0.99      0.99      0.99      2373\n",
            "\n",
            "Matriz de Confusión:\n",
            "[[   0    9    0]\n",
            " [   0 2351    0]\n",
            " [   0    8    5]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}